---
title: "Twitter textual data: scraping and analyzing"
author: "Renato Russo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The 3 big tasks behind analyzing textual Twitter data
### 1. Scrape data
The first thing you should do is obtaining data. Fortunately, Twitter makes it 
relatively easy to scrape data, all you need to start is a Twitter Developer
Account so you can get access to Twitter API. Actually, there are a few different
types of Developer Account. For the purpose of this project, I am using my 
Academic Research account. You may want to check out [Twitter's Developer Portal](https://developer.twitter.com/)
for more information.
From this point on, I will assume that you have your Twitter API credentials at
hand ;-)

#### Packages needed/recommended
There are a few packages that help us retrieve data from Twitter. For this project,
I am using [academictwitteR](https://www.research.ed.ac.uk/en/publications/academictwitter-an-r-package-to-access-the-twitter-academic-resea),
but you may also want to check out [twitteR](https://cran.r-project.org/web/packages/twitteR/twitteR.pdf)
and [rtweet](https://cran.rstudio.com/web/packages/rtweet/index.html). I have already
experienced a few glitches with the former,  seems to be updated less
frequently than rtweet, and the reason why I am using academictwitteR now is because
it has a more straight forward connection with Twitter API V2,  we can use
to retrieve "historical data" (the previous API allowed retrieval of data from
the past 8-10 days only, among other limitations).
So, let's get on with the search:
```{r academictwitter install}
# install.packages("academictwitteR")
library(academictwitteR)
```
#### Set up credentials
For this step, you will have to look at your profile on Twitter's developer portal.
Now, go ahead and get your credentials.
With academictwitteR, I can use the Bearer Token only. The [API documentation](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens)
and the [package repository](https://github.com/cjbarrie/academictwitteR) have
more information on that. For now, you have to call the function `set_bearer()` to
store that information for future use.
```{r bearer}
set_bearer()
```
Once you follow the instructions above, you are ready to go ahead and run your
query!

#### Search what you want
Now, you're finally able to run the search. For that, you will use the
`get_all_tweets()` function. In my case, I am interested in investigating how
sectors of the alt-right have "hijacked" narratives related to the Capitol attack
in January 2021, to assess what type of vocabulary people were using to refer to
different narratives related to that event.
I decided to test how this approach would work with a relatively large corpus of 
tweets, enabled by the affordances of the new Academic Research account.
```{r data collection}
# capitol <- get_all_tweets(query = "capitol", # search query
#                                    start_tweets = "2021-01-06T00:00:00Z", # search start date
#                                    end_tweets = "2021-01-06T19:00:00Z", # search end date
#                                    bearer_token = get_bearer(), # pulling stored bearer toke
#                                    data_path = "data", #where data is stored as series of JSON files
#                                    n = 50000, # total number of tweets retrieved
#                                    is_retweet = FALSE) # excluding retweets
# 
# save(capitol, file = "capitol.Rda")
load("capitol.Rda")

```
### 2. Prepare data
For this analysis, we will predominantly use the "text" column, that is, the 
content of the actual tweet. We could use other data, for example number of likes,
or the timeline of a user, but let's focus on the content for now.

#### Packages needed
```{r package install}
library(tidytext)
library(dplyr)
```
#### Cleaning the data
The function below is used to remove capitalization, and to remove numbers and 
punctuation:
```{r data cleaning}
# First, we define the function
clean_text <- function(text) {
  text <- tolower(text)
  text <- gsub("[[:digit:]]+", "", text)
  text <- gsub("[[:punct:]]+", "", text)
  return(text)
}
# Then, apply it to the text content:
capitol$text <- clean_text(capitol$text)

```
We also want to remove stop words, that is, those that do not add meaning to
textual analysis. Before doing that, we will tokenize the words:

#### Tokenize words
```{r token}
capitol_tokens <- capitol %>%
  tidytext::unnest_tokens(word, text) %>% 
  count(id, word)

capitol_tokens <- capitol_tokens %>% 
  dplyr::anti_join(tidytext::get_stopwords())

```
Previous plotting showed that the word "capitol" was causing noise in the charts
because it obviously appears in every tweet, so I decided to remove it from the
corpus. I understand that this does not harm the analysis, because we are not
interested in the connections with the word, but in the general meaning of tweets
containing that word.
```{r}
my_stopwords <- tibble(word = c(as.character(1:10),
                                    "capitol", "just", "now",
                                "right now"))
capitol_tokens <- capitol_tokens %>% 
  anti_join(my_stopwords)

```
I will also "stem" words, that is, combine words that share the same "root." This
is important because preliminary analysis showed the existence of words that have
similar seemantic value like "storm" and "storming"
```{r stemming}
library(corpus)
capitol_tokens$word <- text_tokens(capitol_tokens$word, stemmer = "en") # english stemmer

```
#### Document-term matrix
The document-term matrix (dtm) is "a mathematical matrix that describes the frequency
of terms that occur in a collection of documents" [3]. In this case, each tweet
is stored as a document, and each word in the tweets are a term. In other words,
the matrix shows whether or not a word appears in a tweet.
```{r DTM}
DTM <- capitol_tokens %>%
  tidytext::cast_dtm(id, word, n)
DTM
```
This specific dtm has 17,635 documents (tweets) and 36,273 terms (unique words).

The dtm will be used for exploratory analysis in the next section.

### 3. Analyze data
#### Exploratory analysis
##### Looking at the structure of the document-term matrix:
```{r }
capitol_tokens %>% 
  group_by(word) %>% 
  summarize(occurrence = sum(n)) %>% 
  arrange(desc(occurrence))

```

#### Natural language processing
```{r topic modeling}
library(tm)
```
##### Visualize topiccs
```{r}
library(topicmodels)
```
## Latent Drichlet Allocation (LDA)
Latent Dirichlet Allocation is an approach to statistical topic modeling in which
documents are represented as a set of topics and a topic is a set of words. The
topics are situated in a "latent layer." Put in a very simple way, this type of
modeling compares the presence of a term in a document and in a topic, and uses
that comparison to establish the probability that the document is part of the topic.
This type of modeling is used here to identify the topics and the co-occurring
terms, that is, terms that appear together in the tweets.
The code below performs the topic allocation. Initial runs of this and the following
parts revealed that the topics were too similar, so I proceeded with the creation
of only 2, which seemed to capture the meaning in tweets satisfactorily.
```{r}
LDA <- topicmodels::LDA(DTM,
                        k = 2, # number of topics
                        control = list(seed = 123))
View(LDA)
LDA_td <- tidytext::tidy(LDA) #tidying the LDA object
LDA_td
```
##### Visualize co-occurring terms
The code below crates a tibble containing topics of 6 terms each.
```{r}
library(tidytext)
topTerms <- LDA_td %>% 
  group_by(topic) %>% 
  top_n(5, beta) %>% # 5 main topics that will later be plotted
  arrange(topic, -beta) # arranging topics in descending order of beta coefficient
topTerms

topTerms %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_x") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Topics in tweets containing the term 'capitol' in January 6, 2021")
```
```{r}
library(widyr)
word_pairs_capitol <- capitol_tokens %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE)
word_pairs_capitol
```
## Including Plots
```{r}
#network of co-occurring words
library(igraph)
library(ggraph)

word_pairs_capitol %>%
  filter(n >= 180) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.5, "lines")) +
  theme_void() +
  labs(title = "Co-occurring terms in tweets containing the term 'capitol' in January 6, 2021")


```

References:
https://jtr13.github.io/cc21/twitter-sentiment-analysis-in-r.html#word-frequency-plot
https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
[3] DTM: https://bookdown.org/Maxine/tidy-text-mining/tidying-a-document-term-matrix.html

<!-- #### Frequency of tweets -->
<!-- Did the number of tweets increase during the hours leading to the Capitol attack? -->
<!-- # ```{r frequency of tweets} -->
<!-- # # this part of the code has been adapted to accommodate changes in the last version -->
<!-- # # of Twitter API. It appears originally in Chapter 33 of last year's Community -->
<!-- # # Contribution (https://jtr13.github.io/cc21/twitter-sentiment-analysis-in-r.html) -->
<!-- # library(lubridate) -->
<!-- # library(stringr) -->
<!-- # capitol_dyor %>%  -->
<!-- #   mutate( -->
<!-- #     created_at = created_at %>%  -->
<!-- #       # Remove zeros. -->
<!-- #       str_remove_all(pattern = '\\+0000') %>% -->
<!-- #       # Parse date. -->
<!-- #       parse_date_time(orders = '%y-%m-%d %H%M%S') -->
<!-- #   ) -->
<!-- #  -->
<!-- # # capitol_dyor %>%  -->
<!-- # #   mutate(Created_At_Round = created_at %>% -->
<!-- # #            round(units = 'hours') %>% -->
<!-- # #            as.POSIXct()) -->
<!-- #  -->
<!-- # # tweets.df %>% pull(created) %>% min() -->
<!-- # library(ggplot2) -->
<!-- # plt <- capitol_dyor %>%  -->
<!-- #   dplyr::count(created_at) %>%  -->
<!-- #   ggplot(mapping = aes(x = created_at, y = n)) + -->
<!-- #   theme_light() + -->
<!-- #   geom_line() + -->
<!-- #   xlab(label = 'Date') + -->
<!-- #   ylab(label = NULL) + -->
<!-- #   ggtitle(label = 'Number of Tweets per Hour') -->
<!-- # library(plotly) -->
<!-- #  -->
<!-- # plt -->

<!-- ``` -->






<!-- #### Wordcloud -->
<!-- ##### Create DTM -->
<!-- Before you create a wordcloud, you need a document-term matrix: -->
<!-- ```{r dtm} -->
<!-- library(wordcloud) -->
<!-- tweets_words <-  capitol %>% -->
<!--   select(text) %>% -->
<!--   unnest_tokens(word, text) -->

<!-- words <- capitol_tokens %>% -->
<!--   count(word, sort=TRUE) -->

<!-- words -->

<!-- set.seed(1234) # for reproducibility -->
<!-- wordcloud(words$word, words$n, min.freq = 50, -->
<!--           max.words=150, random.order=FALSE, rot.per=0.35, -->
<!--           colors=brewer.pal(8, "Dark2")) -->

<!-- #  -->
<!-- library(tm) -->
<!-- # install.packages("wordcloud") -->
<!-- library(wordcloud) -->
<!-- set.seed(1234) # for reproducibility -->

<!-- # wordcloud(words = dyor_tokens$word, freq = dyor_tokens$n, min.freq = 50, -->
<!-- #           max.words=200, random.order=FALSE, rot.per=0.35, -->
<!-- #           colors=brewer.pal(8, "Dark2")) -->
<!-- ``` -->

